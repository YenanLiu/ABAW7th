# # wandb setting

wandb_name: "Data_VA_DDP_s10w10b30_Dlr[1e-5_1e-5]"
AU: False
EXPR: False
VA_Arousal: True
VA_Valence: True
VA: False

AU_weight: 1
EXPR_weight: 1
VA_Arousal_weight: 1
VA_Valence_weight: 1

# data augs
aug_ways: ["RandomHorizontal", "RandomResizedCrop", "RandomRotation", "ColorJitter", "RandomAffine"]

# Combine feature choice
AU_Fea: "AU/val_features.h5"
EXPR_Fea: "EXPR/val_features.h5"
V_Fea: "V/val_features.h5"

train_anno_file: "Track1_five_fold/train_fold_5.csv"
val_anno_file: "Track1_five_fold/val_fold_5.csv"
train_data_dir: "MTL/cropped_aligned/"
val_data_dir: "MTL/cropped_aligned/"

seq_len: 30
win_len: 30
epoch: 30
batchsize: 10  
train_shuffle: False

# log setting
log_dir: "/output/ABAW/MTL_E"

# # data
img_size: 224
num_workers: 4

pretrain_model: "model-50.pth"
finetune_model: ""
resume_model: ""

# training
seed: 1234
optimizer: "AdamW"
# for MAE
MAE_init_lr: 1e-5
# for head and temporal convergence model
head_lr: 1e-5
temporal_lr: 1e-5

scheduler: "CosineAnnealingLR"

# SGD
sgd_momentum: 0.9
sgd_weight_decay: 1e-4
# AdamW
adamW_weight_decay: 0.05 #1e-4

# StepLR
steplr_size: 50
steplr_gamma: 0.1
# MultiStepLR
Msteplr_size: [15, 30, 45, 60]
Msteplr_gamma: 0.1
# ExponentialLR
ExponentialLR_gamma: 0.95
# CosineAnnealingLR
CosineAnnealingLR_T_max: 2
CosineAnnealingLR_eta_min: 1e-5

time_model: "LSTM" # ["Transformer", "LSTM", "TCN", "GRU"]
input_dim: 768

# Transformer hypeparams
d_model: 768
num_heads: 4
num_layers: 2
dropout: 0.3

# LSTM hypeparams
hidden_dim: 768
l_num_layers: 2

# TCN hypeparams
kernel_size: 2
dropout: 0.2
num_channels : [768, 512, 256]

# TCN hypeparams
kernel_size: 2
dropout: 0.2
num_channels : [768, 512, 256]


# for testing
model_weight: "/project/_liuchen/ABAW7th/pth/AU_Repeat_epoch2_best.pth"
prediction_save_dir: "/root/code/ABAW/test_results/"